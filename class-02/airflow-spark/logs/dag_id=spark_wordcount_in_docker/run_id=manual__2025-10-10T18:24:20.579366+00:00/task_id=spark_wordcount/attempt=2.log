[2025-10-10T18:26:44.079+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-10-10T18:26:44.091+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_wordcount_in_docker.spark_wordcount manual__2025-10-10T18:24:20.579366+00:00 [queued]>
[2025-10-10T18:26:44.095+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_wordcount_in_docker.spark_wordcount manual__2025-10-10T18:24:20.579366+00:00 [queued]>
[2025-10-10T18:26:44.095+0000] {taskinstance.py:2865} INFO - Starting attempt 2 of 2
[2025-10-10T18:26:44.100+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): spark_wordcount> on 2025-10-10 18:24:20.579366+00:00
[2025-10-10T18:26:44.103+0000] {standard_task_runner.py:72} INFO - Started process 218 to run task
[2025-10-10T18:26:44.106+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_wordcount_in_docker', 'spark_wordcount', 'manual__2025-10-10T18:24:20.579366+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/spark_dag.py', '--cfg-path', '/tmp/tmpbcgi6scn']
[2025-10-10T18:26:44.108+0000] {standard_task_runner.py:105} INFO - Job 17: Subtask spark_wordcount
[2025-10-10T18:26:44.141+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_wordcount_in_docker.spark_wordcount manual__2025-10-10T18:24:20.579366+00:00 [running]> on host d5e286fcaf27
[2025-10-10T18:26:44.187+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='spark_wordcount_in_docker' AIRFLOW_CTX_TASK_ID='spark_wordcount' AIRFLOW_CTX_EXECUTION_DATE='2025-10-10T18:24:20.579366+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-10T18:24:20.579366+00:00'
[2025-10-10T18:26:44.187+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-10-10T18:26:44.226+0000] {docker.py:355} INFO - Starting docker container from image apache/spark:3.5.1-python3
[2025-10-10T18:26:44.231+0000] {docker.py:363} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2025-10-10T18:26:45.709+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SparkContext: Running Spark version 3.5.1
[2025-10-10T18:26:45.711+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-10-10T18:26:45.711+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SparkContext: Java version 11.0.22
[2025-10-10T18:26:45.749+0000] {docker.py:66} INFO - 25/10/10 18:26:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-10-10T18:26:45.798+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO ResourceUtils: ==============================================================
[2025-10-10T18:26:45.798+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-10-10T18:26:45.798+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO ResourceUtils: ==============================================================
[2025-10-10T18:26:45.799+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SparkContext: Submitted application: WordCount
[2025-10-10T18:26:45.814+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-10-10T18:26:45.821+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO ResourceProfile: Limiting resource is cpu
[2025-10-10T18:26:45.822+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-10-10T18:26:45.859+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SecurityManager: Changing view acls to: spark
[2025-10-10T18:26:45.860+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SecurityManager: Changing modify acls to: spark
[2025-10-10T18:26:45.860+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SecurityManager: Changing view acls groups to:
[2025-10-10T18:26:45.860+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SecurityManager: Changing modify acls groups to:
[2025-10-10T18:26:45.861+0000] {docker.py:66} INFO - 25/10/10 18:26:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2025-10-10T18:26:46.011+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO Utils: Successfully started service 'sparkDriver' on port 35705.
[2025-10-10T18:26:46.027+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO SparkEnv: Registering MapOutputTracker
[2025-10-10T18:26:46.048+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO SparkEnv: Registering BlockManagerMaster
[2025-10-10T18:26:46.058+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-10-10T18:26:46.059+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-10-10T18:26:46.060+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-10-10T18:26:46.073+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9960201a-7634-41cf-bd69-494ab1ed141e
[2025-10-10T18:26:46.084+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-10-10T18:26:46.092+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-10-10T18:26:46.170+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-10-10T18:26:46.215+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-10-10T18:26:46.272+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO Executor: Starting executor ID driver on host 875b5a9b503b
[2025-10-10T18:26:46.272+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-10-10T18:26:46.273+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO Executor: Java version 11.0.22
[2025-10-10T18:26:46.276+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-10-10T18:26:46.277+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4edbb2ab for default.
[2025-10-10T18:26:46.292+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45525.
[2025-10-10T18:26:46.292+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO NettyBlockTransferService: Server created on 875b5a9b503b:45525
[2025-10-10T18:26:46.293+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-10-10T18:26:46.297+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 875b5a9b503b, 45525, None)
[2025-10-10T18:26:46.299+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManagerMasterEndpoint: Registering block manager 875b5a9b503b:45525 with 434.4 MiB RAM, BlockManagerId(driver, 875b5a9b503b, 45525, None)
[2025-10-10T18:26:46.300+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 875b5a9b503b, 45525, None)
[2025-10-10T18:26:46.301+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 875b5a9b503b, 45525, None)
[2025-10-10T18:26:46.745+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)
[2025-10-10T18:26:46.774+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)
[2025-10-10T18:26:46.776+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 875b5a9b503b:45525 (size: 32.6 KiB, free: 434.4 MiB)
[2025-10-10T18:26:46.781+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO SparkContext: Created broadcast 0 from textFile at <unknown>:0
[2025-10-10T18:26:46.861+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO FileInputFormat: Total input files to process : 1
[2025-10-10T18:26:46.930+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO SparkContext: Starting job: sortBy at /app/wordcount.py:26
[2025-10-10T18:26:46.947+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /app/wordcount.py:26) as input to shuffle 0
[2025-10-10T18:26:46.950+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO DAGScheduler: Got job 0 (sortBy at /app/wordcount.py:26) with 2 output partitions
[2025-10-10T18:26:46.950+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO DAGScheduler: Final stage: ResultStage 1 (sortBy at /app/wordcount.py:26)
[2025-10-10T18:26:46.950+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
[2025-10-10T18:26:46.951+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
[2025-10-10T18:26:46.956+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /app/wordcount.py:26), which has no missing parents
[2025-10-10T18:26:46.994+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 434.1 MiB)
[2025-10-10T18:26:46.995+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 434.1 MiB)
[2025-10-10T18:26:46.995+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 875b5a9b503b:45525 (size: 7.6 KiB, free: 434.4 MiB)
[2025-10-10T18:26:46.996+0000] {docker.py:66} INFO - 25/10/10 18:26:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-10-10T18:26:47.004+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /app/wordcount.py:26) (first 15 tasks are for partitions Vector(0, 1))
[2025-10-10T18:26:47.004+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
[2025-10-10T18:26:47.032+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (875b5a9b503b, executor driver, partition 0, PROCESS_LOCAL, 7644 bytes)
[2025-10-10T18:26:47.033+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (875b5a9b503b, executor driver, partition 1, PROCESS_LOCAL, 7644 bytes)
[2025-10-10T18:26:47.042+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-10-10T18:26:47.042+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2025-10-10T18:26:47.083+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO HadoopRDD: Input split: file:/app/data/input.txt:0+44
[2025-10-10T18:26:47.083+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO HadoopRDD: Input split: file:/app/data/input.txt:44+45
[2025-10-10T18:26:47.582+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonRunner: Times: total = 324, boot = 278, init = 45, finish = 1
[2025-10-10T18:26:47.586+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonRunner: Times: total = 322, boot = 275, init = 47, finish = 0
[2025-10-10T18:26:47.601+0000] {docker.py:66} INFO - [Stage 0:>                                                          (0 + 2) / 2]
[2025-10-10T18:26:47.603+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1667 bytes result sent to driver
[2025-10-10T18:26:47.604+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1667 bytes result sent to driver
[2025-10-10T18:26:47.609+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 586 ms on 875b5a9b503b (executor driver) (1/2)
[2025-10-10T18:26:47.612+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 579 ms on 875b5a9b503b (executor driver) (2/2)
[2025-10-10T18:26:47.613+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-10-10T18:26:47.613+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 59513
[2025-10-10T18:26:47.621+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /app/wordcount.py:26) finished in 0.654 s
[2025-10-10T18:26:47.621+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: looking for newly runnable stages
[2025-10-10T18:26:47.622+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: running: Set()
[2025-10-10T18:26:47.622+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: waiting: Set(ResultStage 1)
[2025-10-10T18:26:47.622+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: failed: Set()
[2025-10-10T18:26:47.622+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at sortBy at /app/wordcount.py:26), which has no missing parents
[2025-10-10T18:26:47.627+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.8 KiB, free 434.1 MiB)
[2025-10-10T18:26:47.629+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.1 MiB)
[2025-10-10T18:26:47.629+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 875b5a9b503b:45525 (size: 6.7 KiB, free: 434.4 MiB)
[2025-10-10T18:26:47.629+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-10-10T18:26:47.631+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at sortBy at /app/wordcount.py:26) (first 15 tasks are for partitions Vector(0, 1))
[2025-10-10T18:26:47.631+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2025-10-10T18:26:47.635+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (875b5a9b503b, executor driver, partition 0, NODE_LOCAL, 7433 bytes)
[2025-10-10T18:26:47.635+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (875b5a9b503b, executor driver, partition 1, NODE_LOCAL, 7433 bytes)
[2025-10-10T18:26:47.636+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
[2025-10-10T18:26:47.638+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
[2025-10-10T18:26:47.674+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Getting 2 (194.0 B) non-empty blocks including 2 (194.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-10T18:26:47.675+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Getting 2 (226.0 B) non-empty blocks including 2 (226.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-10T18:26:47.677+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2025-10-10T18:26:47.677+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2025-10-10T18:26:47.721+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonRunner: Times: total = 42, boot = -236, init = 278, finish = 0
[2025-10-10T18:26:47.724+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonRunner: Times: total = 42, boot = -237, init = 279, finish = 0
[2025-10-10T18:26:47.724+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2054 bytes result sent to driver
[2025-10-10T18:26:47.725+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2054 bytes result sent to driver
[2025-10-10T18:26:47.725+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 90 ms on 875b5a9b503b (executor driver) (1/2)
[2025-10-10T18:26:47.727+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 94 ms on 875b5a9b503b (executor driver) (2/2)
[2025-10-10T18:26:47.728+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-10-10T18:26:47.730+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: ResultStage 1 (sortBy at /app/wordcount.py:26) finished in 0.103 s
[2025-10-10T18:26:47.731+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-10T18:26:47.731+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-10-10T18:26:47.732+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Job 0 finished: sortBy at /app/wordcount.py:26, took 0.802048 s
[2025-10-10T18:26:47.733+0000] {docker.py:66} INFO - 
[2025-10-10T18:26:47.756+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO SparkContext: Starting job: sortBy at /app/wordcount.py:26
[2025-10-10T18:26:47.757+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Got job 1 (sortBy at /app/wordcount.py:26) with 2 output partitions
[2025-10-10T18:26:47.757+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Final stage: ResultStage 3 (sortBy at /app/wordcount.py:26)
[2025-10-10T18:26:47.757+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
[2025-10-10T18:26:47.757+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Missing parents: List()
[2025-10-10T18:26:47.757+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at sortBy at /app/wordcount.py:26), which has no missing parents
[2025-10-10T18:26:47.759+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)
[2025-10-10T18:26:47.760+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)
[2025-10-10T18:26:47.760+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 875b5a9b503b:45525 (size: 6.5 KiB, free: 434.3 MiB)
[2025-10-10T18:26:47.761+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-10-10T18:26:47.761+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at sortBy at /app/wordcount.py:26) (first 15 tasks are for partitions Vector(0, 1))
[2025-10-10T18:26:47.762+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0
[2025-10-10T18:26:47.763+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (875b5a9b503b, executor driver, partition 0, NODE_LOCAL, 7433 bytes) 
[2025-10-10T18:26:47.764+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (875b5a9b503b, executor driver, partition 1, NODE_LOCAL, 7433 bytes)
[2025-10-10T18:26:47.765+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Running task 1.0 in stage 3.0 (TID 5)
[2025-10-10T18:26:47.770+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)
[2025-10-10T18:26:47.771+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Getting 2 (194.0 B) non-empty blocks including 2 (194.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-10T18:26:47.772+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-10-10T18:26:47.782+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Getting 2 (226.0 B) non-empty blocks including 2 (226.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-10T18:26:47.782+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2025-10-10T18:26:47.817+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonRunner: Times: total = 42, boot = -45, init = 87, finish = 0
[2025-10-10T18:26:47.817+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 2161 bytes result sent to driver
[2025-10-10T18:26:47.818+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 56 ms on 875b5a9b503b (executor driver) (1/2)
[2025-10-10T18:26:47.827+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonRunner: Times: total = 42, boot = -57, init = 99, finish = 0
[2025-10-10T18:26:47.828+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Finished task 1.0 in stage 3.0 (TID 5). 2240 bytes result sent to driver
[2025-10-10T18:26:47.829+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 65 ms on 875b5a9b503b (executor driver) (2/2)
[2025-10-10T18:26:47.830+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
[2025-10-10T18:26:47.830+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: ResultStage 3 (sortBy at /app/wordcount.py:26) finished in 0.073 s
[2025-10-10T18:26:47.830+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-10T18:26:47.831+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-10-10T18:26:47.832+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Job 1 finished: sortBy at /app/wordcount.py:26, took 0.076584 s
[2025-10-10T18:26:47.871+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
[2025-10-10T18:26:47.874+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-10-10T18:26:47.874+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-10-10T18:26:47.874+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-10-10T18:26:47.903+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
[2025-10-10T18:26:47.903+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Registering RDD 9 (sortBy at /app/wordcount.py:26) as input to shuffle 1
[2025-10-10T18:26:47.903+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Got job 2 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions
[2025-10-10T18:26:47.904+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Final stage: ResultStage 6 (runJob at SparkHadoopWriter.scala:83)
[2025-10-10T18:26:47.904+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2025-10-10T18:26:47.905+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
[2025-10-10T18:26:47.907+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /app/wordcount.py:26), which has no missing parents
[2025-10-10T18:26:47.908+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.0 KiB, free 434.1 MiB)
[2025-10-10T18:26:47.910+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 434.1 MiB)
[2025-10-10T18:26:47.912+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 875b5a9b503b:45525 (size: 7.3 KiB, free: 434.3 MiB)
[2025-10-10T18:26:47.912+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-10-10T18:26:47.912+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /app/wordcount.py:26) (first 15 tasks are for partitions Vector(0, 1))
[2025-10-10T18:26:47.913+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2025-10-10T18:26:47.913+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (875b5a9b503b, executor driver, partition 0, NODE_LOCAL, 7422 bytes)
[2025-10-10T18:26:47.913+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (875b5a9b503b, executor driver, partition 1, NODE_LOCAL, 7422 bytes)
[2025-10-10T18:26:47.913+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
[2025-10-10T18:26:47.914+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
[2025-10-10T18:26:47.922+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Getting 2 (194.0 B) non-empty blocks including 2 (194.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-10T18:26:47.922+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-10-10T18:26:47.927+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Getting 2 (226.0 B) non-empty blocks including 2 (226.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-10T18:26:47.927+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-10-10T18:26:47.967+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonRunner: Times: total = 42, boot = -102, init = 144, finish = 0
[2025-10-10T18:26:47.972+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2226 bytes result sent to driver
[2025-10-10T18:26:47.975+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 63 ms on 875b5a9b503b (executor driver) (1/2)
[2025-10-10T18:26:47.976+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO PythonRunner: Times: total = 42, boot = -98, init = 140, finish = 0
[2025-10-10T18:26:47.978+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2226 bytes result sent to driver
[2025-10-10T18:26:47.979+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 67 ms on 875b5a9b503b (executor driver) (2/2)
[2025-10-10T18:26:47.980+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-10-10T18:26:47.982+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: ShuffleMapStage 5 (sortBy at /app/wordcount.py:26) finished in 0.075 s
[2025-10-10T18:26:47.982+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: looking for newly runnable stages
[2025-10-10T18:26:47.983+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: running: Set()
[2025-10-10T18:26:47.983+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: waiting: Set(ResultStage 6)
[2025-10-10T18:26:47.983+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: failed: Set()
[2025-10-10T18:26:47.983+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[16] at saveAsTextFile at <unknown>:0), which has no missing parents
[2025-10-10T18:26:47.994+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 109.1 KiB, free 434.0 MiB)
[2025-10-10T18:26:47.997+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 41.0 KiB, free 433.9 MiB)
[2025-10-10T18:26:47.997+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 875b5a9b503b:45525 (size: 41.0 KiB, free: 434.3 MiB)
[2025-10-10T18:26:47.998+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-10-10T18:26:47.999+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[16] at saveAsTextFile at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-10T18:26:47.999+0000] {docker.py:66} INFO - 25/10/10 18:26:47 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-10-10T18:26:48.002+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (875b5a9b503b, executor driver, partition 0, NODE_LOCAL, 7727 bytes) 
[2025-10-10T18:26:48.003+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
[2025-10-10T18:26:48.024+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
[2025-10-10T18:26:48.024+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-10-10T18:26:48.024+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-10-10T18:26:48.028+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO ShuffleBlockFetcherIterator: Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-10T18:26:48.028+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-10-10T18:26:48.074+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO PythonRunner: Times: total = 42, boot = -57, init = 99, finish = 0
[2025-10-10T18:26:48.075+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO ShuffleBlockFetcherIterator: Getting 2 (217.0 B) non-empty blocks including 2 (217.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-10T18:26:48.075+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-10-10T18:26:48.121+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO PythonRunner: Times: total = 42, boot = 0, init = 42, finish = 0
[2025-10-10T18:26:48.122+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO PythonRunner: Times: total = 97, boot = -55, init = 152, finish = 0
[2025-10-10T18:26:48.130+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO FileOutputCommitter: Saved output of task 'attempt_202510101826476591521425089679494_0016_m_000000_0' to file:/app/data/out/_temporary/0/task_202510101826476591521425089679494_0016_m_000000
[2025-10-10T18:26:48.131+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO SparkHadoopMapRedUtil: attempt_202510101826476591521425089679494_0016_m_000000_0: Committed. Elapsed time: 2 ms.
[2025-10-10T18:26:48.133+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 2351 bytes result sent to driver
[2025-10-10T18:26:48.134+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 133 ms on 875b5a9b503b (executor driver) (1/1)
[2025-10-10T18:26:48.134+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-10-10T18:26:48.135+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO DAGScheduler: ResultStage 6 (runJob at SparkHadoopWriter.scala:83) finished in 0.150 s
[2025-10-10T18:26:48.135+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-10T18:26:48.135+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-10-10T18:26:48.137+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO DAGScheduler: Job 2 finished: runJob at SparkHadoopWriter.scala:83, took 0.233623 s
[2025-10-10T18:26:48.137+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO SparkHadoopWriter: Start to commit write Job job_202510101826476591521425089679494_0016.
[2025-10-10T18:26:48.150+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO SparkHadoopWriter: Write Job job_202510101826476591521425089679494_0016 committed. Elapsed time: 11 ms.
[2025-10-10T18:26:48.150+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-10-10T18:26:48.156+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO SparkUI: Stopped Spark web UI at http://875b5a9b503b:4040
[2025-10-10T18:26:48.163+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-10-10T18:26:48.175+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO MemoryStore: MemoryStore cleared
[2025-10-10T18:26:48.176+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO BlockManager: BlockManager stopped
[2025-10-10T18:26:48.181+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-10-10T18:26:48.188+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-10-10T18:26:48.195+0000] {docker.py:66} INFO - 25/10/10 18:26:48 INFO SparkContext: Successfully stopped SparkContext
[2025-10-10T18:26:49.179+0000] {docker.py:66} INFO - 25/10/10 18:26:49 INFO ShutdownHookManager: Shutdown hook called
[2025-10-10T18:26:49.180+0000] {docker.py:66} INFO - 25/10/10 18:26:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-23666b9e-5921-4dd2-85ad-61e569ee57bb
[2025-10-10T18:26:49.181+0000] {docker.py:66} INFO - 25/10/10 18:26:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7921c0f-37ce-4b40-9993-64dffe604ad0/pyspark-1da39c34-0ae6-4b7f-b9ea-60d8ac7d61c8
[2025-10-10T18:26:49.183+0000] {docker.py:66} INFO - 25/10/10 18:26:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7921c0f-37ce-4b40-9993-64dffe604ad0
[2025-10-10T18:26:49.332+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-10-10T18:26:49.333+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=spark_wordcount_in_docker, task_id=spark_wordcount, run_id=manual__2025-10-10T18:24:20.579366+00:00, execution_date=20251010T182420, start_date=20251010T182644, end_date=20251010T182649
[2025-10-10T18:26:49.358+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-10-10T18:26:49.368+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-10-10T18:26:49.369+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
